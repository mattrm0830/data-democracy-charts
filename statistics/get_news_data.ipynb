{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import openai\n",
    "import ssl\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Create directories if they don't exist\n",
    "Path(\"statistics\").mkdir(exist_ok=True)\n",
    "Path(\"visualizations\").mkdir(exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def get_news_articles(query, days_back=30):\n",
    "    \"\"\"\n",
    "    Fetch news articles from NewsAPI.ai\n",
    "    \"\"\"\n",
    "    base_url = \"https://newsapi.ai/api/v4/article/getArticles\"\n",
    "    \n",
    "    # Calculate date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days_back)\n",
    "    \n",
    "    # Format dates as required by NewsAPI.ai\n",
    "    date_start = start_date.strftime('%Y-%m-%d')\n",
    "    date_end = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Build the body for the POST request\n",
    "    request_body = {\n",
    "        \"action\": \"getArticles\",\n",
    "        \"keyword\": query,\n",
    "        \"articlesSortBy\": \"date\",\n",
    "        \"articlesSortByAsc\": False,\n",
    "        \"articlesPage\": 1,\n",
    "        \"articlesCount\": 100,\n",
    "        \"articleBodyLen\": -1,  # Get full article body\n",
    "        \"resultType\": \"articles\",\n",
    "        \"dataType\": [\"news\", \"blog\"],\n",
    "        \"apiKey\": NEWS_API_KEY,\n",
    "        \"forceMaxDataTimeWindow\": 31,\n",
    "        \"lang\": \"eng\",\n",
    "        \"dateStart\": date_start,\n",
    "        \"dateEnd\": date_end\n",
    "    }\n",
    "    \n",
    "    all_articles = []\n",
    "    page = 1\n",
    "    max_pages = 10  # Limit to prevent excessive API usage\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        request_body[\"articlesPage\"] = page\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nMaking request to NewsAPI.ai with query: {query}, page: {page}\")\n",
    "            \n",
    "            # Using POST instead of GET for NewsAPI.ai\n",
    "            response = requests.post(base_url, json=request_body)\n",
    "            \n",
    "            print(f\"Response status code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error response: {response.text[:500]}...\")\n",
    "                break\n",
    "                \n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract articles from the response\n",
    "            articles = data.get('articles', {}).get('results', [])\n",
    "            \n",
    "            if not articles:\n",
    "                print(\"No more articles found\")\n",
    "                break\n",
    "                \n",
    "            total_articles = data.get('articles', {}).get('totalResults', 0)\n",
    "            print(f\"Found {len(articles)} articles on page {page} (Total available: {total_articles})\")\n",
    "            \n",
    "            # Transform the articles to match our expected format\n",
    "            transformed_articles = []\n",
    "            for article in articles:\n",
    "                source_name = article.get('source', {}).get('title', 'Unknown')\n",
    "                \n",
    "                # Extract the text content\n",
    "                body = article.get('body', '')\n",
    "                title = article.get('title', '')\n",
    "                description = article.get('description', '') or article.get('extract', '')\n",
    "                \n",
    "                transformed_articles.append({\n",
    "                    'title': title,\n",
    "                    'description': description,\n",
    "                    'body': body,\n",
    "                    'url': article.get('url', ''),\n",
    "                    'source': {'name': source_name},\n",
    "                    'publishedAt': article.get('dateTime', datetime.now().isoformat()),\n",
    "                    'date': article.get('date', datetime.now().strftime('%Y-%m-%d'))\n",
    "                })\n",
    "            \n",
    "            all_articles.extend(transformed_articles)\n",
    "            \n",
    "            # If we've reached the end of available articles\n",
    "            if len(articles) < request_body[\"articlesCount\"]:\n",
    "                print(f\"Reached the end of available articles at page {page}\")\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "            # Respect API rate limits with a short delay\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            break\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            print(f\"Response text: {response.text[:500]}...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Total articles collected: {len(all_articles)}\")\n",
    "    return all_articles\n",
    "\n",
    "# Function to analyze political leaning using OpenAI\n",
    "def analyze_political_leaning(text):\n",
    "    \"\"\"\n",
    "    Analyze the political leaning of a text using OpenAI's GPT-4\n",
    "    \"\"\"\n",
    "    if not text or len(text.strip()) < 10:\n",
    "        print(\"Text too short for analysis, returning neutral score\")\n",
    "        return 0\n",
    "        \n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a political analyst. Analyze the following text and determine its political leaning on a scale from -10 (very liberal/left) to 10 (very conservative/right), where 0 is neutral/centrist. Respond with only the numerical score.\"},\n",
    "                {\"role\": \"user\", \"content\": text[:800]}  # Using more context for better analysis\n",
    "            ],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        score_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Handle potential non-numeric responses\n",
    "        try:\n",
    "            score = float(score_text)\n",
    "            normalized_score = max(min(score, 10), -10)  # Ensure within -10 to 10 range\n",
    "            return normalized_score\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert response to float: '{score_text}'\")\n",
    "            return 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing political leaning: {e}\")\n",
    "        return 0  # Default to neutral if analysis fails\n",
    "\n",
    "# Extract state mentions from article\n",
    "def extract_states(text):\n",
    "    \"\"\"\n",
    "    Extract mentions of US states from article text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "        \n",
    "    # List of US states and their abbreviations\n",
    "    states = [\n",
    "        \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \n",
    "        \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \n",
    "        \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \n",
    "        \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \n",
    "        \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \n",
    "        \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \n",
    "        \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\",\n",
    "        \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \n",
    "        \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "        \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \n",
    "        \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    "    ]\n",
    "    \n",
    "    # Map of abbreviations to full names\n",
    "    abbr_to_name = {\n",
    "        \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\n",
    "        \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\",\n",
    "        \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\",\n",
    "        \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\",\n",
    "        \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\", \"MO\": \"Missouri\",\n",
    "        \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\",\n",
    "        \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\",\n",
    "        \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\n",
    "        \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\",\n",
    "        \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"\n",
    "    }\n",
    "    \n",
    "    mentioned_states = []\n",
    "    \n",
    "    # Check each potential state name in the text\n",
    "    # Using case-insensitive word boundary search to avoid false positives\n",
    "    for state in states:\n",
    "        # Check if state appears as a whole word\n",
    "        if f\" {state} \" in f\" {text} \" or f\" {state},\" in text or f\" {state}.\" in text:\n",
    "            if len(state) == 2:  # It's an abbreviation\n",
    "                state_name = abbr_to_name.get(state)\n",
    "                if state_name and state_name not in mentioned_states:\n",
    "                    mentioned_states.append(state_name)\n",
    "            elif state not in mentioned_states:\n",
    "                mentioned_states.append(state)\n",
    "    \n",
    "    return mentioned_states\n",
    "\n",
    "# Main processing function\n",
    "def compile_news_political_data():\n",
    "    \"\"\"\n",
    "    Compile news data with political leaning analysis by state\n",
    "    \"\"\"\n",
    "    # Get articles for multiple topics to ensure broad coverage\n",
    "    topics = [\"politics\", \"election\", \"governor\", \"senator\", \"congress\", \"state legislation\"]\n",
    "    all_articles = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        print(f\"\\n=== Fetching articles for topic: {topic} ===\")\n",
    "        articles = get_news_articles(topic)\n",
    "        if articles:\n",
    "            print(f\"Found {len(articles)} articles for topic: {topic}\")\n",
    "            all_articles.extend(articles)\n",
    "        else:\n",
    "            print(f\"No articles found for topic: {topic}\")\n",
    "    \n",
    "    if not all_articles:\n",
    "        print(\"No articles were fetched. Please check your API key and try again.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "    \n",
    "    # Remove duplicates based on URL\n",
    "    unique_urls = set()\n",
    "    unique_articles = []\n",
    "    for article in all_articles:\n",
    "        if article['url'] not in unique_urls:\n",
    "            unique_urls.add(article['url'])\n",
    "            unique_articles.append(article)\n",
    "    \n",
    "    print(f\"\\nProcessing {len(unique_articles)} unique articles...\")\n",
    "    \n",
    "    # Process each article\n",
    "    processed_data = []\n",
    "    for i, article in enumerate(unique_articles):\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Processing article {i+1}/{len(unique_articles)}\")\n",
    "            \n",
    "        # Get the most comprehensive text available for analysis\n",
    "        title = article.get('title', '')\n",
    "        description = article.get('description', '')\n",
    "        body = article.get('body', '')\n",
    "        \n",
    "        # Combine available text for the most complete content\n",
    "        full_text = title\n",
    "        if description:\n",
    "            full_text += \" \" + description\n",
    "        if body:\n",
    "            full_text += \" \" + body\n",
    "        \n",
    "        # Extract states mentioned\n",
    "        states = extract_states(full_text)\n",
    "        \n",
    "        # Only process if states are mentioned\n",
    "        if states:\n",
    "            # Analyze political leaning\n",
    "            leaning = analyze_political_leaning(full_text)\n",
    "            \n",
    "            # Add to processed data\n",
    "            for state in states:\n",
    "                processed_data.append({\n",
    "                    'state': state,\n",
    "                    'title': title,\n",
    "                    'url': article['url'],\n",
    "                    'source': article.get('source', {}).get('name', 'Unknown'),\n",
    "                    'political_leaning': leaning,\n",
    "                    'date': article.get('date', datetime.now().strftime('%Y-%m-%d'))\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Save the processed data\n",
    "        df.to_csv('statistics/news_political_data.csv', index=False)\n",
    "        print(f\"\\nSaved {len(df)} processed articles with state mentions to statistics/news_political_data.csv\")\n",
    "    else:\n",
    "        print(\"\\nNo articles with state mentions were found.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate visualizations\n",
    "def generate_visualizations(df):\n",
    "    \"\"\"\n",
    "    Generate visualizations from the processed news data\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    # Create a directory for visualizations if it doesn't exist\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    \n",
    "    # 1. Prepare data: Aggregate political leanings by state\n",
    "    state_leanings = df.groupby('state')['political_leaning'].mean().reset_index()\n",
    "    state_leanings['normalized_leaning'] = state_leanings['political_leaning'] / 10  # Normalize to -1 to 1 scale\n",
    "    \n",
    "    # Sort by political leaning\n",
    "    state_leanings_sorted = state_leanings.sort_values('political_leaning')\n",
    "    \n",
    "    # Add counts for each state\n",
    "    state_counts = df['state'].value_counts().reset_index()\n",
    "    state_counts.columns = ['state', 'article_count']\n",
    "    state_leanings = state_leanings.merge(state_counts, on='state', how='left')\n",
    "    \n",
    "    # 2. Bar chart of political leanings by state\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    ax = sns.barplot(x='political_leaning', y='state', data=state_leanings_sorted, \n",
    "                    palette=\"coolwarm\", hue='political_leaning', dodge=False)\n",
    "    \n",
    "    # Add count annotations\n",
    "    for i, row in state_leanings_sorted.iterrows():\n",
    "        ax.text(0, i, f\" n={row['article_count']}\", va='center')\n",
    "    \n",
    "    plt.axvline(x=0, color='purple', linestyle='--')\n",
    "    plt.xlabel('Political Leaning (Left [-10] to Right [+10])')\n",
    "    plt.ylabel('State')\n",
    "    plt.title('Average Political Leaning of News Articles by State')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/state_political_leanings.png', dpi=300)\n",
    "    print(\"Saved state political leanings bar chart\")\n",
    "    \n",
    "    # 3. Create a US map visualization\n",
    "    try:\n",
    "        # Set SSL context to unverified for the geojson download\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        \n",
    "        # Load US states map\n",
    "        usa = gpd.read_file('https://raw.githubusercontent.com/PublicaMundi/MappingAPI/master/data/geojson/us-states.json')\n",
    "        \n",
    "        # Merge the political leaning data with the map data\n",
    "        merged = usa.merge(state_leanings, left_on='name', right_on='state', how='left')\n",
    "        \n",
    "        # Create a choropleth map\n",
    "        fig, ax = plt.subplots(1, figsize=(16, 10))\n",
    "        \n",
    "        # Add data to map\n",
    "        merged.plot(column='political_leaning', cmap='coolwarm', linewidth=0.8, ax=ax, edgecolor='0.8',\n",
    "                   legend=True, legend_kwds={'label': \"Political Leaning (Left [-10] to Right [+10])\", \n",
    "                                            'orientation': \"horizontal\"}, \n",
    "                   missing_kwds={'color': 'lightgrey'})\n",
    "        \n",
    "        # Add state names and article counts as annotations\n",
    "        for idx, row in merged.iterrows():\n",
    "            if pd.notna(row['political_leaning']):\n",
    "                plt.annotate(text=f\"{row['name']}\\n(n={row['article_count']})\", \n",
    "                            xy=(row['geometry'].centroid.x, row['geometry'].centroid.y),\n",
    "                            horizontalalignment='center', fontsize=8)\n",
    "            else:\n",
    "                plt.annotate(text=f\"{row['name']}\\n(no data)\", \n",
    "                            xy=(row['geometry'].centroid.x, row['geometry'].centroid.y),\n",
    "                            horizontalalignment='center', fontsize=8, color='grey')\n",
    "        \n",
    "        plt.title('Political Leaning of News Coverage by State')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/us_map_political_leanings.png', dpi=300)\n",
    "        print(\"Saved US map visualization\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating map visualization: {e}\")\n",
    "    \n",
    "    # 4. Article count by state\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.barplot(x='article_count', y='state', \n",
    "               data=state_counts.sort_values('article_count', ascending=False).head(20))\n",
    "    plt.xlabel('Number of Mentions in Articles')\n",
    "    plt.ylabel('State')\n",
    "    plt.title('Top 20 States by News Coverage')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/state_mention_counts.png', dpi=300)\n",
    "    print(\"Saved article count visualization\")\n",
    "    \n",
    "    # 5. Scatter plot of political leaning vs coverage\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.scatterplot(data=state_leanings, x='political_leaning', y='article_count', \n",
    "                   size='article_count', sizes=(20, 500), hue='political_leaning', \n",
    "                   palette='coolwarm')\n",
    "    \n",
    "    # Add state labels to points\n",
    "    for i, row in state_leanings.iterrows():\n",
    "        plt.annotate(row['state'], (row['political_leaning'], row['article_count']), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.axvline(x=0, color='purple', linestyle='--')\n",
    "    plt.xlabel('Political Leaning (Left [-10] to Right [+10])')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.title('Relationship Between Political Leaning and News Coverage by State')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/leaning_vs_coverage.png', dpi=300)\n",
    "    print(\"Saved political leaning vs coverage scatter plot\")\n",
    "    \n",
    "    print(\"\\nAll visualizations saved to the 'visualizations' directory!\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting news political leaning analysis...\")\n",
    "    df = compile_news_political_data()\n",
    "    if not df.empty:\n",
    "        generate_visualizations(df)\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\nNo data to visualize. Please check your API key and try again.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
